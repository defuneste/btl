[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "A short list of my recent presentations:\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nOct 20, 2024\n\n\nAUBER 2024\n\n\nFCC, broadband, package, data as code\n\n\n\n\nOct 18, 2023\n\n\nURISA 2023\n\n\nFCC, Quarto, URISA, broadband\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-03-01-sampling-allong-a-line/index.html",
    "href": "posts/2021-03-01-sampling-allong-a-line/index.html",
    "title": "Sampling Along a Line",
    "section": "",
    "text": "Recently, someone posted a question on Linkedin about sampling along some lines. It may not be obvious for everyone why anyone would like to do that so let’s provide some examples: sampling in a river every N meters, sampling species diversity along transects. The lines or segments can be used as a way to measure some kind of gradient or to follow something that can be represented as a line. A line is just a one-dimensional (1D) representation, so even if we use it in space it can represent way more stuff… like time!\nThe question had to do with QGIS and Postgis. I was a bit curious about how to do it in R and also wanted try with QGIS (later!). I used rocker/geospatial:4.0.4 with sf (0.9.0) and spastat (1.63-3) packages (and their respective dependencies). My exploration went a bit longer than expected, so I will just start with spastat then keep going."
  },
  {
    "objectID": "posts/2021-03-01-sampling-allong-a-line/index.html#pointsonlines",
    "href": "posts/2021-03-01-sampling-allong-a-line/index.html#pointsonlines",
    "title": "Sampling Along a Line",
    "section": "pointsOnLines",
    "text": "pointsOnLines\nWhile looking for how to make a regular sample in a line in a spatstat, I found pointsOnLines with this nice documentation page. This looks perfect for us. Let’s try it!\nThe first line just creates some lines (“randomly”). The only difficulty is the psp function. This function creates an object of class psp or a line segment pattern. Spatstat uses its own objects and methods but it is easy to convert them as you can see in the second line where the sf package is used to write our dear friend shapefile. I commented the line because I didn’t want to run a new set of lines every time.\n\nsome_lines &lt;- spatstat.geom::psp(runif(20), runif(20), runif(20), runif(20),  window = spatstat.geom::owin()) \n# sf::st_write(sf::st_as_sf(some_lines)[-1,], \"some_lines.shp\")\nlines &lt;- sf::st_read(\"some_lines.shp\")\n\nReading layer `some_lines' from data source \n  `/home/defuneste/Documents/btl/posts/2021-03-01-sampling-allong-a-line/some_lines.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 20 features and 1 field\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0.02009798 ymin: 0.03753605 xmax: 0.9965629 ymax: 0.9619593\nCRS:           NA\n\n\n\n#\\ label: reading them back \nsome_lines &lt;- spatstat.geom::as.psp(sf::st_geometry(lines), window = owin()) #  read them back\nplot(some_lines, main = \"Draw some lines\", col = \"grey\")\n\n\n\n\n\n\n\n\nI admit that indexing with negative 1 ([-1,]) is a bit ugly, but if you want to use the almighty shapefile you can only use one type of geometry (no one is perfect!). That’s why you have to remove the polygon that is the bbox at the first line (or use some more hipster geo format). Finally, in the third line, we read it back. Spatstat works primarily in projected coordinates and this should be kept in mind (we don’t care too much about our lack of CRS here, which would not be the case in a real-world project which could fail if you use geographic coordinate systems).\nNow we can use the pointsOnLines function to create an object of ppp type (AKA points pattern) and plot the points. We just use two arguments, the first one is our lines and the second eps is Spacing between successive points. As stated in the details section of the documentation the spacing of eps is measured in coordinate units of X”. Here we used some numbers (20) from a uniform distribution with the default arguments (min = 0 and max = 1) so eps = 0.1 was fine. I also liked to shortok default argument so I didn’t change it.\n\nsampling_points &lt;- spatstat.geom::pointsOnLines(some_lines, eps = 0.1)\nplot(some_lines, main=\"Add points!\", col = \"grey\")\nplot(sampling_points, add = TRUE, pch = \"+\", col = \"darkred\")\n\n\n\n\n\n\n\n\n\nsf::st_write(sf::st_as_sf(sampling_points)[-1,], \"sampling_points.shp\") # save them"
  },
  {
    "objectID": "posts/2021-03-01-sampling-allong-a-line/index.html#where-is-the-first-sample-placed-on",
    "href": "posts/2021-03-01-sampling-allong-a-line/index.html#where-is-the-first-sample-placed-on",
    "title": "Sampling Along a Line",
    "section": "Where is the first sample placed on?",
    "text": "Where is the first sample placed on?\nOne question remains! Where should the first point in a line be placed? This is trickier than it looks: what happens when the length of a lines is not a multiple of our spacing value? Where should we start placing points?\nWe can start with a simple example : just one line. We didn’t set up any units but to make it simple, let’s use meters (m).\n\nfenetre &lt;- spatstat.geom::owin(xrange=c(-1,11), yrange=c(0,2)) # define a reusable window \nline &lt;- spatstat.geom::psp(0, 1, 10, 1,  window = fenetre)     # one line length = 10\nguide &lt;- spatstat.geom::ppp(0:10, rep(1,11), window = fenetre) # create some guides\nmid &lt;- spatstat.geom::ppp(0.5:9.5, rep(1,10), window = fenetre)# create more guides\nsamples &lt;- spatstat.geom::pointsOnLines(line, eps = 3)         # spacing of 3\nplot(line, col = \"grey\", main = \"\")                       # plot everything \nplot(guide, pch = \"|\", add = TRUE)\nplot(mid, pch = \"+\", add = TRUE, col = \"grey\")\nplot(samples, pch = 16, col = \"darkred\", add = TRUE)\n\n\n\n\n\n\n\n\nWell this innocuous question bring us something unexpected. We expected 3 samples but we got 4 and it looks like the first sample, on both sides, is located at 1 m from the end/beginning of the line (in a circular line they will be 2 m apart) but something is staring to be problematic : the only spacing of 3 is between the second and third sample, other spacing is 2.5 m. If we take a longer line (25 m like below) we get the same pattern (1m, 2.5m then 3) but with more samples spaced with 3 m.\n\nfenetre &lt;- spatstat.geom::owin(xrange=c(-1,26), yrange=c(0,2)) # define a reusable windows \nline &lt;- spatstat.geom::psp(0, 1, 25, 1,  window = fenetre)     # one line length = 10\nguide &lt;- spatstat.geom::ppp(0:25, rep(1,26), window = fenetre) # create some guides\nsamples &lt;- spatstat.geom::pointsOnLines(line, eps = 3)         # spacing of 3\nplot(line, col = \"grey\", main = \"\")                       # plot everything \nplot(guide, pch = \"|\", add = TRUE)\nplot(samples, pch = 16, col = \"darkred\", add = TRUE)\n\n\n\n\n\n\n\n\nLet’s dive into the code (thx open source!). I will skip the part that defines the function, check what arguments are used, etc.. and do a first stop at L25-L32. This part is the code for what should be done if we have too lines that are too small (length &lt;= spacing) and shortok == TRUE (our setup). We can see that with this option, a sample point will be added at the middle of our small line.\nI will then move to the part for every other (i.e., non-small) line. We can also skip the loop and the bind part and just try with one line. Below I have just reproduced L36 and 47 :\n\n# re using same example \nfenetre &lt;- spatstat.geom::owin(xrange=c(-1,11), yrange=c(0,2))\nline &lt;- spatstat.geom::psp(0, 1, 10, 1,  window = fenetre) \nlinedf    &lt;- as.data.frame(line)\neps &lt;- 3                                    # some spacing\n\nleni &lt;- spatstat.geom::lengths_psp(line)    # length of the segment : 10 \nnwhole &lt;- floor(leni/eps)                   # how many whole segments can we fit : 3\nif(leni/eps - nwhole &lt; 0.5 && nwhole &gt; 2)   # if we do 3 nwhole we have some \"leftovers\"\n    nwhole &lt;- nwhole - 1                    # I guess it is a design choice \nrump &lt;- (leni - nwhole * eps)/2             # \nbrks &lt;- c(0, rump + (0:nwhole) * eps, leni) # making bricks: 0 2 5 8 10\nnbrks &lt;- length(brks)                       # how many bricks: 5\n# points at middle of each piece\nss &lt;- (brks[-1] + brks[-nbrks])/2\ntp &lt;- ss/leni                               # make it relative\nx &lt;- with(linedf, x0 + tp * (x1-x0))        # give them coordinates\ny &lt;- with(linedf, y0 + tp * (y1-y0))\n\nThe authors have made some design choices. Instead of going for 3 points, they have gone with 4 and focused on their definition : “Given a line segment pattern, place a series of points at equal distances along each line segment.”. Points are not located at every x spacing but instead at equal distances: if we set 3 points they are not at the same distance anymore, so it seems they opted to make a compromise, as when (L9) adds a point and decreases the spacing a bit and when not doing so. I feel this is a correct choice when sampling and it remind me a lot of reflections about the all block quadra variance family and the importance of the starting point."
  },
  {
    "objectID": "posts/2021-03-01-sampling-allong-a-line/index.html#conclusion",
    "href": "posts/2021-03-01-sampling-allong-a-line/index.html#conclusion",
    "title": "Sampling Along a Line",
    "section": "Conclusion",
    "text": "Conclusion\nSpatstat is definitely a statistical package at heart and you have to be thoughtful when you use it a bit outside of its main goal (as we figured out)! If you want to use it, you will have to adjust it a bit or remove the first measures at the ends of the lines.\nNext time, let’s see how sf handles this case."
  },
  {
    "objectID": "posts/2021-03-01-sampling-allong-a-line/index.html#footnotes",
    "href": "posts/2021-03-01-sampling-allong-a-line/index.html#footnotes",
    "title": "Sampling Along a Line",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://spatstat.org/2020/12/14/spatstat-split.html↩︎"
  },
  {
    "objectID": "posts/2024-07-14-from-elephant-to-duck/index.html",
    "href": "posts/2024-07-14-from-elephant-to-duck/index.html",
    "title": "From elephant to duck!",
    "section": "",
    "text": "With John Hall we wrote a blog post about moving a table from postgres to a parquet file using duckdb.\n\nThere are a lot of conversations — understandably — on the use of Apache Parquet, Apache Arrow and DuckDB. […] Let’s see an example of how they can be used to convert a table in a PostgresSQL database to a parquet file:\n\nYou can find the post on CORI’s Map and data analytics team blog."
  },
  {
    "objectID": "posts/2024-03-10-awesome-jq/index.html",
    "href": "posts/2024-03-10-awesome-jq/index.html",
    "title": "Awesome jq and GeoJSON",
    "section": "",
    "text": "I wrote a blog post about using jq for filtering geojson.\n\nIf you are manipulating a lot of GeoJSON features/objects and want a quick CLI tool to filter and slice them, you should give jq a try! Since there are not many tutorials that exist on using jq to manage objects in the GeoJSON family, we hope that these few tricks will help you on your learning journey.\n\nYou can find the post on CORI’s Map and data analytics team blog."
  },
  {
    "objectID": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html",
    "href": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html",
    "title": "The Wonderfull World of Overpass Turbo",
    "section": "",
    "text": "I use Overpass Turbo a lot and every time, I rediscover new amazing stuff to do with it. This is one of the many gems in the OpenStreetMap ecosystem! In this post, I will assume a basic understanding of OpenStreetMap (OSM) data structure, but if you want a quick quality refresher, I really like the one in Overpass API’s User Manual."
  },
  {
    "objectID": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html#what-is-overpass-turbo",
    "href": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html#what-is-overpass-turbo",
    "title": "The Wonderfull World of Overpass Turbo",
    "section": "What is Overpass Turbo?",
    "text": "What is Overpass Turbo?\nOverpass Turbo is a frontend for Overpass API. The author is Martin Raifer, the code and the list of contributors can be browsed on the project’s GitHub. You can find the attributions for the entire stack at the bottom of this post.\nOverpass API merits several posts. Here, I will mostly focus on Overpass Turbo (but maybe you will learn some nice stuff with Overpass API!). It’s main contributor/author is Roland Olbricht and the code can be found here.\nIf you want to use OSM data you you can follow two main paths (I don’t like using the OSM editing API to retrieve data). The first one uses .pbf files1. You can check osmextract if you are interested in that option and want use R. GEOFABRIK is providing PBF files for different parts of the world at various scales. You will also probably either need Osmosis and/or osm2pgsql. The second method for data retrieval uses Overpass.\nOverpass API will store all the data behind the map and a diff file with last minute changes that happened subsequently can be added. This API is read only: you will not be able to write into OSM with it. For that you should use other kinds of tools, like JOSM or OSM API (and the old OSM XAPI).\nOverpass Turbo is the perfect place to prototype your queries and explore OSM data."
  },
  {
    "objectID": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html#a-tour-of-overpass-turbo",
    "href": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html#a-tour-of-overpass-turbo",
    "title": "The Wonderfull World of Overpass Turbo",
    "section": "A Tour of Overpass Turbo:",
    "text": "A Tour of Overpass Turbo:\nI will first repeat some basic informations from OSM’s Wiki.\nYou can divide the overpass turbo interface in 3 parts.\n\nIn A we have the Editor panel. In this picture, I used one from the tool examples. As I already said, I will not dive too deeply in Overpass Query Language (Overpass QL), but I’ll explain this one:\n/*\nUse this to comment on multi lines  \n*/\nnode                        // Part 1: here we want nodes matching\n  [amenity=drinking_water]  // amenity=drinking_water   \n  ({{bbox}});               // inside the bbox displayed in B \n                            // \";\" -&gt; end of statement               \nout;                        // Part 2: we take part 1 and display it  \nB is the display panel. The result of your query can be displayed in the “Map” or directly in “Data” (top right corner). Here we are in XML because out wasn’t specified. To specify another output format, you need to add a setting ([out:json]) before node see Settings in Overpass QL wiki.\n\nLastly we can find various menus (C) on the top:\n\nRun executes the query\nShare provides you a link with your query, the bbox used, and can execute it\nExport gives you various ways to export the data, the map and the query (I will show you some nice tips later)\nWizard helps you to build your first query. See its wiki for more details\nSave/Load enables you to save and load your own queries (Load also contains very nice examples! Check out the one by MapCSS!)\nSetting can help you fine tune Overpass turbo and export your settings\nHelp provides you with plenty of good links and the attributions (I used it)"
  },
  {
    "objectID": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html#getting-data",
    "href": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html#getting-data",
    "title": "The Wonderfull World of Overpass Turbo",
    "section": "Getting data!",
    "text": "Getting data!\nNow that you are more familiar with Overpass turbo let’s use the Wizard to help us make our request. I typed river then pressed build query:\n\n\n\n\n\n\n\n\n\nThe wizard has built a first query. After a quick check, this query looks fine. I will just increase the time-out a bit from 25 to 360 (it is in seconds). I can do that because this query will be run in a local instance of Overpass API (on my computer), not on the public server. I will explain the limitations of public servers a bit later.\nThis is the query:\n/*\nThis has been generated by the overpass-turbo wizard.\nThe original search was:\n“river”\n*/\n[out:json][timeout:360];\n// gather results\n(\n  // query part for: “river”\n  way[\"natural\"=\"water\"][\"water\"=\"river\"]({{bbox}});\n  relation[\"natural\"=\"water\"][\"water\"=\"river\"]({{bbox}});\n);\n// print results\nout body;\n&gt;;\nout skel qt;                     \nAs a result, I get 15323 nodes making 112 ways, 10 relations and 72 polygons. You can see that on the bottom right part of the Map panel. You can easily explore the data while moving around the map. If you are happy with the result, you can export it in a GeoJSON via the Export menu, then by selecting Data and here picking GeoJSON. Overpass Turbo offers two options: either download it or copy it to clipboard.\nMost of time you will need to improve the query to match your needs. For that, OSM wiki contains a lot of useful information. I checked the river entry in it and discovered that the key waterways can be interesting to explore.\n\nSaving our Bounding Box (bbox)\nYou may have noticed that we used the screen display to set up our bounding box(bbox). This is nice but if we want to reproduce our workflow or keep the same bounding box with other tools/data, we need to record it. You have various ways to save it.\nThe first one is to go in the Export menu, then go to the Map tab and select Current Map view. The bbox is in “Bounds” (with other useful information).\n\n\n\n\n\n\n\n\n\nThe second option is to get the bbox from the xml or from the QL. You can get them from the same Export menu in the Query then you can pick convert to Overpass-XML or pick OverpassQL (compact or not). If you have downloaded Overpass API you can find this convert script in cgi-bin/ directory of the program. Converting to a compact Overpass Query Language is very useful when you need to send multiple requests on an overpass api instance.\nI have made two quick R functions to retrieve this informations:\n\n# from xml\nbbox_from_xml &lt;-function(file_xml) {\n    all_xml = xml2::read_xml(file_xml)\n    bbox_query = xml2::xml_find_first(all_xml, \".//bbox-query\")\n    bbox_string = xml2::xml_attrs(bbox_query)\n    as.numeric(c(bbox_string[\"w\"], bbox_string[\"s\"], bbox_string[\"e\"], bbox_string[\"n\"]))\n}\n\n# from overpass AL\nbbox_from_QL &lt;- function(file_ql) {\n    raw_ql = readLines(file_ql)\n    # the ugly regex to get the start of bbox\n    # I keep ) so I will need to remove it \n    locate_bbox = regexpr(pattern = \"[0-9].+?(?=\\\\))\", \n                           raw_ql, perl = TRUE)\n    bbox_string = substring(raw_ql, \n                             locate_bbox[1], \n                             locate_bbox[1] + attr(locate_bbox, \"match.length\") - 1 )\n    as.numeric(unlist(strsplit(bbox_string, split = \",\"))) # round a bit \n}\n\nOSM bounding box follows the “min Longitude, min Latitude, max Longitude, max Latitude” but that’s not the case of the overpass API which follows “South, West, North East” (ie: min Latitude, min Longitude, max Latitude, max Longitude). You start with south then go clockwise.\nYou can use osmdata::bbox_to_string to convert it to a string. I also highly recommend that you use the osmdata2 package alongside Overpass Turbo, both work very well together.\nWhen you have a specific bbox, you can also use it in Overpass QL settings3.\n\n\nUsing instances other than the default\nOverpass Turbo is the frontend of an instance of the Overpass API hosted somewhere. You have a list here of all the public instances available. You can find them in the settings menu, then General Settings in the server pull-down menu.\nThose instances are provided by the community and to avoid someone taking over all the shared resources, some rules need to be respected. This is why you have a time-out set when you start a query. If, like me, you want to experiment or use a good amount of server resources, you should set up your own instance (or at least start with querying a small amount of data/a less complex request) .\nWhen doing that, you can provide your private address to Overpass Turbo. I use a pretty standard installation of the instance on my computer so mine is http://localhost:80/api/ (yup localhost on port 80 behind an apache web server).\nBonus: if you use osmdata, you can use your own OSM instance with set_overpass_url()\nI hope you discovered some useful stuff! If I miss something or if you want to correct/add things, feel free to reach out!\n\n\nFull attributions\n\nData Sources\nData © OpenStreetMap contributors, ODbL (Terms)\nData mining by Overpass API\nMap tiles © OpenStreetMap contributors, CC-BY-SA\nSearch provided by Nominatim\n\n\nSoftware & Libraries\nMap powered by Leaflet\nEditor powered by CodeMirror\nOther libraries: osmtogeojson, togpx, tokml, lodash, jQuery, jQuery UI, html2canvas, canvg, leaflet-locationfilter, leaflet.PolylineOffset, maki, SJJB map icons, Osmic, FileSaver.js, MapBBCode, Moment.js, polylabel, osm-auth"
  },
  {
    "objectID": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html#footnotes",
    "href": "posts/2021-10-23-wonderfull-of-overpass-turbo/index.html#footnotes",
    "title": "The Wonderfull World of Overpass Turbo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsee also https://en.wikipedia.org/wiki/Protocol_Buffers↩︎\nMark Padgham, Bob Rudis, Robin Lovelace, Maëlle Salmon (2017). osmdata Journal of Open Source Software, 2(14). URL https://doi.org/10.21105/joss.00305↩︎\nsee here for instructions↩︎"
  },
  {
    "objectID": "posts/2021-01-29-sensor-observation-standard/index.html",
    "href": "posts/2021-01-29-sensor-observation-standard/index.html",
    "title": "Sensor Observation Service: an Introduction",
    "section": "",
    "text": "Thanks to SIST and the financial support of my lab, I was able to attend a training course on Sensor Observation Services (SOS) with one of my coworkers. It’s time to reflect a bit on what we learned!"
  },
  {
    "objectID": "posts/2021-01-29-sensor-observation-standard/index.html#observations",
    "href": "posts/2021-01-29-sensor-observation-standard/index.html#observations",
    "title": "Sensor Observation Service: an Introduction",
    "section": "Observations",
    "text": "Observations\nThis network monitors noise in various places. Places are called Features of Interest (FOI), featureOfInterest in XML. One of them is here urn:ogc:def:feature:x-istsos:1.0:Point:Berthelot. This is more simple that it looks: we have an Uniform Resource Name from OGC that defines a feature in istSOS (the service we will use) version 1.0. This feature is a point and has the name (identifier) of Berthelot. Now we would like to collect observations about noise here. To do that, we will use a procedure : a way to collect observations. The procedure has a location (coordinates with a coordinate reference system) Here this is done with one sensor but it can be done with more than one or/and using some processes. Our FOI have the procedure/sensor AF01 associated (urn:ogc:def:procedure:x-istsos:1.0:AF01). It is monitoring at least one observed property (observedProperty). AF01 produce 4 of them (lday, levening, lnight). lday is a noise level, taking into account human ears in decibels, calculated during daylight hours (6am-6pm). Eventually we will get a result, let’s say 62.8 dB(A). dB(A) is our unit of measure (uom). Time is usually expressed in ISO 8601 (2021-02-06T06:00:00.000000Z).\nTogether, all of the above make an observation!\nFinally, offering enables you to make some logical grouping of observations."
  },
  {
    "objectID": "posts/2021-01-29-sensor-observation-standard/index.html#making-use-of-what-we-learn",
    "href": "posts/2021-01-29-sensor-observation-standard/index.html#making-use-of-what-we-learn",
    "title": "Sensor Observation Service: an Introduction",
    "section": "Making use of what we learn",
    "text": "Making use of what we learn\nThe schema below is adapted from the OGC documentation (p. 14 and 15). It shows the different steps required to collect data with an associate request. First, we need to know what kind of services are available.\n\nThis a job for a GetCapabilities request:\nhttps://download.data.grandlyon.com/sos/bruit?service=SOS&request=GetCapabilities\nThe first part is the address: https://download.data.grandlyon.com/sos/bruit\n(bruit means ‘noise’ in French)\nThen, after ? we start the request specifying the service and the request. With this request we get an XML giving us all of the information about the service and what we can do with it. A quick scroll gives us the name of the procedures we can request information from and the output format provided.\nWe can now ask for more information about the sensor with DescribeSensor before getting observations.\nhttps://download.data.grandlyon.com/sos/bruit?request=DescribeSensor&procedure=AF01&outputFormat=text%2Fxml%3Bsubtype%3D%22sensorML%2F1.0.1%22&service=SOS&version=1.0.0\nThis one looks a bit more complicated, so let’s examine it. We will still use the same address. Then, we will request DescribeSensor and we will ask for the only output format available: text/xml;subtype=\"sensorML/1.0.1\". It looks a bit complicated, because some characters need to be escaped (outputFormat=text%2Fxml%3Bsubtype%3D%22sensorML%2F1.0.1%22). The last part is just asking for SOS version 1.0.\nThis request allows us to check the AllowedTimes for our next request : GetObservation.\nhttps://download.data.grandlyon.com/sos/bruit?service=SOS&version=1.0.0&request=GetObservation&offering=observatoire_acoustique_grandlyon&procedure=AF01&eventTime=2010-06-17T08:00:00+02:00/2021-01-23T07:00:00+01:00&observedProperty=lday&responseFormat=text/plain\nOk this result is HUGE! However, we have already built some understanding and I will explain the part that we haven’t seen yet. In SOS version 1.0, we need to specify the offering that groups our sensor (offering=observatoire_acoustique_grandlyon).\nThis information comes from the GetCapabilities. Then, we define our procedure and set up a time interval of interest (eventTime=2010-06-17T08:00:00+02:00/2021-01-23T07:00:00+01:00).\nAfter that, we specify the observed property that we’re interested in (observedProperty=lday) and response format (responseFormat=text/plain). Just as before, these two pieces of insight come from the GetCapabilities. As you may have noticed, we don’t need to add all observed property and procedure definitions, just the identifier is fine (lday vs urn:ogc:def:parameter:::noise:lday).\nMuch more can and should be said (you can use bbox or aggregate in requests!) but let’s stop here. I hope that we can explore some ways to interact with http requests in R, soon!\n\naf01 &lt;- read.csv(\"bruit.csv\", header = FALSE)\naf01$V1 &lt;- lubridate::as_datetime(af01$V1)\nnames(af01) &lt;- c(\"time\", \"sensor\", \"lday\")\nquickplot &lt;- ggplot2::ggplot(data = af01, ggplot2::aes( x =  time, y = lday)) +\n                ggplot2::geom_point(alpha = .1, col = \"darkred\") + \n                ggplot2::ylim(c(50, 90)) +\n                ggplot2::theme_bw()\nquickplot"
  },
  {
    "objectID": "posts/2023-01-05-learning-ds-meme/index.html",
    "href": "posts/2023-01-05-learning-ds-meme/index.html",
    "title": "Learning Data Science Meme",
    "section": "",
    "text": "I know that I should not bother too much about some meme and just enjoy them like some cheap beer (yes, I enjoy cheap beer) but I would like to react to this one:\n/rant on"
  },
  {
    "objectID": "posts/2023-01-05-learning-ds-meme/index.html#where-am-i",
    "href": "posts/2023-01-05-learning-ds-meme/index.html#where-am-i",
    "title": "Learning Data Science Meme",
    "section": "Where am I?",
    "text": "Where am I?\nAs a R user I guess I am probably in one of the slopes :). I do not know Rust, Scala, C++, C#, SAS, etc so I guess I am on the left side of it.\nLike every meme you can find plenty of interpretations (and I like that). Let’s start with something good then we will go with the bad stuff."
  },
  {
    "objectID": "posts/2023-01-05-learning-ds-meme/index.html#good-points",
    "href": "posts/2023-01-05-learning-ds-meme/index.html#good-points",
    "title": "Learning Data Science Meme",
    "section": "Good points",
    "text": "Good points\nI think you should focus on building strong foundations in a few tools/language before learning plenty of them and you should not disperse yourself with superficial knowledge. I assume this is the distinction between the “stupid guy” and the “smart one”."
  },
  {
    "objectID": "posts/2023-01-05-learning-ds-meme/index.html#bad-stuffs",
    "href": "posts/2023-01-05-learning-ds-meme/index.html#bad-stuffs",
    "title": "Learning Data Science Meme",
    "section": "Bad stuffs",
    "text": "Bad stuffs\nI will have fewer problems with the meme if it did not mention IQ (yup I am one of the view guys how read axis label). I feel it imply that the “stupid guy” will never be a smart one while I think this Gaussian is more a journey than some typology of people (based on IQ …). In my case learning a bit of C/C++ will be very helpful because lots of dependencies, I used is in this language (and yes sometimes to solve some issue you need to go that far). I also know a lot of smart people that are learning Rust and improving their programming skill because they are learning new ways and new perspective on programming.\nI guess you could say that, at the end, “senior” people know that using just Python/SQL/Excel is the best way to maximize their ROI. It is probably correct but I think a “senior” or a smart guy is someone that can identify the right tool to solve a problem (and not necessarily using the “second best one”) and to do that they probably learned by experimenting with other tools."
  },
  {
    "objectID": "posts/2023-01-05-learning-ds-meme/index.html#summing-it-up",
    "href": "posts/2023-01-05-learning-ds-meme/index.html#summing-it-up",
    "title": "Learning Data Science Meme",
    "section": "Summing it up",
    "text": "Summing it up\nIf you know SQL/excel and learning Python (or R) you are doing a great job (and you should not feel bad or stupid)! If you enjoy learning Rust, C or Julia and learning something new you are also awesome.\n/rant off"
  },
  {
    "objectID": "posts/2021-12-23-toying-with-de9im/index.html",
    "href": "posts/2021-12-23-toying-with-de9im/index.html",
    "title": "Toying with DE-9IM",
    "section": "",
    "text": "This little text below is from a reply I posted to issue. I tried to help a bit and I learned a lot at the same time!\nThe discussion topic was the st_relate function in the sf great package. this function implements the Dimensionally Extended 9-Intersection Model (DE-9IM) 1 or intersection matrix.\nGeometries can be polygons, lines and points. Polygons, two dimensional objects, are delimited by their boundaries (a line) and they can have an interior (an area) and an exterior (another area). Other geometries will mostly have lower dimensions in boundary and interior.\nThis is a nice representation from wikipedia:\nHere is the matrix of all of the possible options:\nEach part of the matrix can get a value. The value is related to the dimensions of the returned object.\nThis flattened matrix will look like this: “212101212”. This is called a pattern.\nThis matrix can be used in two ways: describe a relation between polygons or specify a type or relation you are interested in. As a result: you can use st_relate in two different ways:"
  },
  {
    "objectID": "posts/2021-12-23-toying-with-de9im/index.html#compute-the-de-9im-relations-between-two-objects",
    "href": "posts/2021-12-23-toying-with-de9im/index.html#compute-the-de-9im-relations-between-two-objects",
    "title": "Toying with DE-9IM",
    "section": "Compute the DE-9IM relations between two objects",
    "text": "Compute the DE-9IM relations between two objects\nTo have an easier time reading the pattern, let’s make a quick function:\n\npattern_de_9im =  \"2FFF1FFF2\"\n\nmatrix_de_9im = function(pattern) {\n    string = unlist(strsplit(pattern , \"\"))\n    matrix_de_9im = matrix(string, nrow = 3, byrow = TRUE)\n    colnames(matrix_de_9im) = c(\"I\", \"B\", \"E\")\n    row.names(matrix_de_9im) = c(\"I\", \"B\", \"E\")\n    return(matrix_de_9im)\n}\n\nmatrix_de_9im(pattern_de_9im)\n\n  I   B   E  \nI \"2\" \"F\" \"F\"\nB \"F\" \"1\" \"F\"\nE \"F\" \"F\" \"2\"\n\n\nI will enhance an example from the sf documentation.\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\n# 4 polygones to start \npol1 = st_polygon(list(rbind(c(0,0),c(1,0),c(1,1),c(0,1),c(0,0)))) - 0.5\npol2 = pol1 + 1\npol3 = pol1 + 1.5\npol4 = st_polygon(list(rbind(c(-1,0.5),c(-1,1.5),c(0,1.5),c(0,0.5),c(-1,0.5))))\n\nsquares = sf::st_sf(data.frame(name = LETTERS[1:4],\n           sf::st_sfc(pol1, pol2, pol3, pol4))\n          )\n\nplot(squares)\n\n\n\n\n\n\n\n\nHere we can see how each square relates to the others:\n\nmy_first_st_relate = sf::st_relate(squares, squares)\ncolnames(my_first_st_relate) = squares$name\nrow.names(my_first_st_relate) = squares$name\nmy_first_st_relate\n\n  A           B           C           D          \nA \"2FFF1FFF2\" \"FF2F01212\" \"FF2FF1212\" \"FF2F11212\"\nB \"FF2F01212\" \"2FFF1FFF2\" \"212101212\" \"FF2FF1212\"\nC \"FF2FF1212\" \"212101212\" \"2FFF1FFF2\" \"FF2FF1212\"\nD \"FF2F11212\" \"FF2FF1212\" \"FF2FF1212\" \"2FFF1FFF2\"\n\n\nWe are producing a symmetrical matrix and you just need to focus on half of it. The first column describes how the A square is related to the other squares.\nFirst line, first column [A,A] describes how the same square relates to itself:\n\nmatrix_de_9im(my_first_st_relate[\"A\",\"A\"])\n\n  I   B   E  \nI \"2\" \"F\" \"F\"\nB \"F\" \"1\" \"F\"\nE \"F\" \"F\" \"2\"\n\n\nThey share the same interior, same boundary and same exterior.\nLet’s see a more complicated case: square B with C:\n\nmatrix_de_9im(my_first_st_relate[\"B\",\"C\"])\n\n  I   B   E  \nI \"2\" \"1\" \"2\"\nB \"1\" \"0\" \"1\"\nE \"2\" \"1\" \"2\"\n\n\nThey share a part of their interior, their interior covers a part of their boundaries, a part of each interior is also an exterior of the other and their boundaries are related on two points.\nFeel free to experiment with the other squares!"
  },
  {
    "objectID": "posts/2021-12-23-toying-with-de9im/index.html#check-matching-patterns",
    "href": "posts/2021-12-23-toying-with-de9im/index.html#check-matching-patterns",
    "title": "Toying with DE-9IM",
    "section": "Check matching patterns",
    "text": "Check matching patterns\nNow that we are familiar with the pattern, we can search the matrix for particular patterns. The fact that patterns are just strings allows us to also use regular expressions or other string tricks. Here, for example, we can return every pair-wise relation that shares a boundary.\n\n# . represent every single character \nmatrix(grepl(pattern = \"....1....\", my_first_st_relate),\n       nrow = 4, byrow = TRUE) \n\n      [,1]  [,2]  [,3]  [,4]\n[1,]  TRUE FALSE FALSE  TRUE\n[2,] FALSE  TRUE FALSE FALSE\n[3,] FALSE FALSE  TRUE FALSE\n[4,]  TRUE FALSE FALSE  TRUE\n\n\nWe can also translate spatial predicates with the DE-9IM pattern: see the Wikipedia page.\nWe can achieve the same result thanks to the pattern argument of the st_relate function. The syntax of the pattern changes a bit, you need to replace . (that means every single character) with * to match DE-9IM rules.\n\nsf::st_relate(squares, squares,  pattern = \"****1****\",  sparse = FALSE)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,]  TRUE FALSE FALSE  TRUE\n[2,] FALSE  TRUE FALSE FALSE\n[3,] FALSE FALSE  TRUE FALSE\n[4,]  TRUE FALSE FALSE  TRUE\n\n\nWith sparse = FALSE it will return the matrix. If you change it to TRUE you get a a sparse geometry binary predicate in the form of a list (cf. chapter 4 of Geocomputation with R)\n\nsf::st_relate(squares, squares,  pattern = \"****1****\",  sparse = TRUE)\n\nSparse geometry binary predicate list of length 4, where the predicate\nwas `relate_pattern'\n 1: 1, 4\n 2: 2\n 3: 3\n 4: 1, 4\n\n\nYou can do a lot with this tool! My commentary on an issue lead Robin to write a nice introduction to DE-9IM."
  },
  {
    "objectID": "posts/2021-12-23-toying-with-de9im/index.html#footnotes",
    "href": "posts/2021-12-23-toying-with-de9im/index.html#footnotes",
    "title": "Toying with DE-9IM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEgenhofer M. J., Herring J. R. (1995) Categorizing binary topoligical relationships between regions, lines, and points in geographic databases. Technical Report, Department of Surveying Engineering, University of Maine, Orono, ME.↩︎\nHsu, L. S., Obe, R. (2021). PostGIS in Action, Third Edition. États-Unis: Manning. p269↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I am a Senior Data Engineer for the Center On Rural Innovation where I help data scientists and software engineers make sense of data and use it to help rural communities throughout America.\nPreviously, I worked as a geographer researching topics related to trees and the environmental, including studies on agroforestry, urban settings, virtual trees, tree identification applications and databases, etc.\nI hold a Ph.D. in Geography and Environmental Studies from the University of Paris.\nBranch Twig Leaf is a blog exploring new approaches to spatial data as well as trees in digital humanities and social science research.\nMastodon"
  },
  {
    "objectID": "talks/urisa2023.html",
    "href": "talks/urisa2023.html",
    "title": "URISA 2023",
    "section": "",
    "text": "The Urban and Regional Information Systems Association — better known as URISA — is the main association for professionals in the GIS and geospatial space.\nWe were fortunate enough to be selected to present a portion of our work on broadband infrastructure at URISA’s annual conference GIS-Pro-2023 last fall in Columbus, Ohio.\nOur presentation, “Rural areas are overrepresented in unserved areas and underestimated in statistics,” utilized FCC NBM data to demonstrate several points:\n\nThe new dataset (NBM) represents an improvement over the previous one (derived from F477).\nDespite the narrowing gap between rural and nonrural areas, significant disparities still exist.\nThe current BEAD definition of an “unserved area” is less suitable for rural areas than for urban areas.\n\nYou can dig more into our presentation HERE!"
  },
  {
    "objectID": "talks/auber2024.html",
    "href": "talks/auber2024.html",
    "title": "AUBER 2024",
    "section": "",
    "text": "AUBER is the Association for University Business and Economic Research, a professional organization for economic research centers at leading universities and affiliated organizations across America.\nI presented a talk on the benefits of streamlining date access using an R Package with cori.data.fcc as an example.\nThis is a short list of the advantages:\n\nAddress Data Challenges\n\nData is packaged as code to simplify data access, reduce errors, and promote collaboration.\nLow-level data transformations are codified: this makes them easy for others to reproduce.\n\nAccelerate Innovation\n\nBroadband data is packaged so that researchers can focus on analysis and insights, not data wrangling.\n\nUnlock Deeper Insights Faster\n\ncori.data.fcc provides fast access to granular details, essential for understanding broadband challenges across multiple geographic scales.\n\n\nMy presentation is available HERE!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Improving my default bash prompt\n\n\n\n\n\n\nshell\n\n\nbash\n\n\nlinux\n\n\ngit\n\n\nCLI\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nCodeberg links and icons!\n\n\n\n\n\n\nCodeberg\n\n\nblog\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nLittle Schemer\n\n\n\n\n\n\nScheme\n\n\nLISP\n\n\nFunctional programming\n\n\n\n\n\n\n\n\n\nNov 3, 2024\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nFunction factories to improve Database read and write\n\n\n\n\n\n\nR\n\n\nPG\n\n\nFunctional programming\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nFrom elephant to duck!\n\n\n\n\n\n\nDuckDB\n\n\nR\n\n\nSQL\n\n\nPG\n\n\n\n\n\n\n\n\n\nJul 14, 2024\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nAwesome jq and GeoJSON\n\n\n\n\n\n\ngeojson\n\n\njq\n\n\nCLI\n\n\nGIS\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nMapscaping: geospatial consulting – as a business and a career\n\n\n\n\n\n\npodcast\n\n\nGIS\n\n\n\n\n\n\n\n\n\nJan 14, 2023\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nR is (also) a graph calculator\n\n\n\n\n\n\nR\n\n\nplot\n\n\nggplot2\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Go Map to contribute to OSM\n\n\n\n\n\n\nOSM\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nLearning Data Science Meme\n\n\n\n\n\n\nlearning\n\n\nmeme\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nToying with DE-9IM\n\n\n\n\n\n\nR\n\n\nsf\n\n\n\nexploring st_relate\n\n\n\n\n\nDec 23, 2021\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nThe Wonderfull World of Overpass Turbo\n\n\n\n\n\n\nOSM\n\n\n\nSome tips for Overpass Turbo\n\n\n\n\n\nOct 23, 2021\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Along a Line\n\n\n\n\n\n\nSampling\n\n\nSpatstat\n\n\n\nfirst part : Spatstat\n\n\n\n\n\nApr 8, 2021\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nSensor Observation Service: an Introduction\n\n\n\n\n\n\nSensor\n\n\nSmart environment\n\n\nTime series\n\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\nOlivier Leroy\n\n\n\n\n\n\n\n\n\n\n\n\nTrees in Open Data Databases in France\n\n\n\n\n\n\nurban forest\n\n\nsurvey\n\n\ntree\n\n\n\n\n\n\n\n\n\nJan 10, 2021\n\n\nOlivier Leroy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-11-03_little_schemer/index.html",
    "href": "posts/2024-11-03_little_schemer/index.html",
    "title": "Little Schemer",
    "section": "",
    "text": "I have been reading and enjoying The Little Schemer! It’s time to write a bit about it and structure what I learned so far (up to chapter 5).\nIf you do not know about it I might start by giving a quick introduction. This book about recursion has a very specific and original way of teaching."
  },
  {
    "objectID": "posts/2024-11-03_little_schemer/index.html#teaching-methods",
    "href": "posts/2024-11-03_little_schemer/index.html#teaching-methods",
    "title": "Little Schemer",
    "section": "Teaching Methods:",
    "text": "Teaching Methods:\nInstead of providing a lengthy text and definition it (nearly) always start with a question then provide with you an answer.\nFor example:\n\nIs it true that this is an atom?\natom\nYes.\nbecause atom is a string of characters beginning with the letter a.\n\nThis is the first definition that you are given. The book uses this style to either introduce you to a definition or give you small tasks and time to think about them.\nThe exception to that practice occurs when the authors introduce “Laws” and “Commandments”.\nExample:\n\nThe Law of Car:\nThe primitive car is defined only for non-empty lists.\n\n“Laws” and “Commandments” are build incrementally, you will start with preliminary version and improve them as we build examples and learn more.\nWhen I want to practice a bit on a computer I am using this version of Scheme and Emacs\nIt is using Scheme (a LISP dialect) but so far I am mostly using pen and paper, which is one reason I like it a lot: you can practice everywhere and do not need a computer.\nBefore building a function, the authors always define how the function should behave (with a series of questions). Now we would probably frame that as Test Driven Development.\nFor example we want to create a function firsts that is taking a list l asan argument:\n(firsts l) where l is:\n((apple peach pumpkin) \n (plum pear cherry)\n (grape raisin pea)\n (bean carrot eggplant))\nShould return: (apple plum grape bean)\nIf the first S-expression of an internal list. is a list it should be returned and if the list is empty it should return an empty list (I am shortcuting you here 3 other questions/answers)."
  },
  {
    "objectID": "posts/2024-11-03_little_schemer/index.html#an-example-on-recursion",
    "href": "posts/2024-11-03_little_schemer/index.html#an-example-on-recursion",
    "title": "Little Schemer",
    "section": "An Example on Recursion:",
    "text": "An Example on Recursion:\nI did not begin this book because of his way of teaching (this was a huge added benefit, I was not aware of) but mostly because I wanted to get better at using recursion.\nHence let’s see how we can define firsts\nThe first step when using a recursion is defining when we should stop it (see First Commandment). Here, we will stop when the list is empty (this also matches our special case as (firsts ()) should return ()). This is the called the termination condition, and describe in the Fourth Commandment: be sure where using a recursion to change an argument that will be tested to stop it.\nThen we need to build the typical element, ie what should be returned (in our previous example apple is one of them). In Scheme it looks like:\n(car (car l))\ncar is a primitive function that returns the first S-expression of an non-empty list ((car l) would have returned (apple peach pumpkin)).\nAfter that we need to provide the rest of the list to our firsts function. LISP has an other primitive function called cdr used for that:\n(firsts (cdr l))\nIn our example this would be ((plum pear cherry) (grape raisin pea) (bean carrot eggplant)) and this is called the natural recursion.\nThe last piece is to patch them together and for that LISP use cons (see the Second Commandment).\n\\[ (cons \\underbrace{(car (car \\quad l))}_{typical \\quad element} \\overbrace{(firsts (cdr \\quad l))}^{natural \\quad recursion}) \\]\nNow we can write the function:\n(define firsts\n  (lambda (l)\n    (cond\n      ((null?) quote()) ;termination condition\n      (else (cons \n              (car (cars l)) ;typical element\n              (firsts (cdr l)) ;natural recursion\n      )))))\nThat’s it!\nI still would like to highlight a few points:\n\nThe termination condition is a good example on why you should check for error first\nAll of the “Commandments” are easy to find in the backcover of the book. I do not think they are meant to be memorized. They will probably make no sense if you do not go over the exercises first!\nThe book always focuses on providing a workable solution:\n\nSometimes the first example is also incorrect and the author goes over and explains how to correct it.\nSometimes it can be improved or simplified1\n\n\nI also like that iterative approach."
  },
  {
    "objectID": "posts/2024-11-03_little_schemer/index.html#footnotes",
    "href": "posts/2024-11-03_little_schemer/index.html#footnotes",
    "title": "Little Schemer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo far, most simplifications I have seen are either using set logic to simplify conditions or building other functions.↩︎"
  },
  {
    "objectID": "posts/2023-01-10-r-is-also-a-graph-calculator/index.html",
    "href": "posts/2023-01-10-r-is-also-a-graph-calculator/index.html",
    "title": "R is (also) a graph calculator",
    "section": "",
    "text": "I am doing some basic maths. I was to frustrate of Roger Bivand explaining stuff that I could not understand with my maths background (science background but loooong time ago!). I find a nice free book and practice with it. A lot can be done with pen + paper but sometimes you need to represent an equation in a Cartesian plane.\nR is perfect for that (even if it is not a Computer algebra system) but I am always forgetting some specific ways to do it!\nFirst we do a simple function:\n\\[y = \\sqrt[3](1 - x^2)\\]\nsimple_function &lt;- function(x){\n    temp = (1 - x*x)\n    # R is using natural log so you need to adjust a bit, ie if you use negative\n    # value you will get NaN :\n    # kind of same idea of doing cube_root(-1) * cube_root(abs(x))\n    sign(temp)*abs(temp)**(1/3)\n}\nsimple_function(-5:5)\n\n [1] -2.884499 -2.466212 -2.000000 -1.442250  0.000000  1.000000  0.000000\n [8] -1.442250 -2.000000 -2.466212 -2.884499\nThen you have (as far as I know) 3 options!"
  },
  {
    "objectID": "posts/2023-01-10-r-is-also-a-graph-calculator/index.html#vector-and-plot",
    "href": "posts/2023-01-10-r-is-also-a-graph-calculator/index.html#vector-and-plot",
    "title": "R is (also) a graph calculator",
    "section": "Vector and plot:",
    "text": "Vector and plot:\nHere this is simple we generate a sequence of values and apply our function on it.\n\nx &lt;- -5:5\ny &lt;- simple_function(x)\nplot(x, y, type = \"b\", col = 2)\n\n\n\n\n\n\n\n# if you prefer it can also go in data frame because both vectors have the same length\n#df &lt;- data.frame(x = x,\n#                 y = simple_function(x))\n\nThis is good but if you pay attention you can see that using type = \"b\" we are basically plotting the point and connecting them with a straight line. What happens if this is not a straight line:\n\nx &lt;- seq(from = -5, to = 5, by = .1)\ny &lt;- simple_function(x)\nplot(x , y, type = \"b\", col = 2)\n\n\n\n\n\n\n\n\nThis is a bit better!"
  },
  {
    "objectID": "posts/2023-01-10-r-is-also-a-graph-calculator/index.html#curve",
    "href": "posts/2023-01-10-r-is-also-a-graph-calculator/index.html#curve",
    "title": "R is (also) a graph calculator",
    "section": "curve!",
    "text": "curve!\nThe base-R packagegraphics provide us with a lot of very cool functions and one of it is curve(). It can take a function or an expression.\n\ncurve(simple_function, from = -5, to = 5,\n      ylab = \"y\", col = 2) # some small tuning is still needed\n\n\n\n\n\n\n\n\nEasy and simple."
  },
  {
    "objectID": "posts/2023-01-10-r-is-also-a-graph-calculator/index.html#ggplot",
    "href": "posts/2023-01-10-r-is-also-a-graph-calculator/index.html#ggplot",
    "title": "R is (also) a graph calculator",
    "section": "ggplot!",
    "text": "ggplot!\nObviously, we can also do it with ggplot::stat_function():\n\nlibrary(ggplot2)\n# I was lazy and just reused x\nggplot(data.frame(x), aes(x = x)) +\n    stat_function(fun = simple_function, colour = 2) +\n    theme_bw()"
  },
  {
    "objectID": "posts/2023-01-06-2-using-go-map-to-contribute-to-osm/index.html",
    "href": "posts/2023-01-06-2-using-go-map-to-contribute-to-osm/index.html",
    "title": "Using Go Map to contribute to OSM",
    "section": "",
    "text": "I was a bit intimidated to contribute to OSM and it took me time to do it. I needed to move to another country, get a correct smartphone (I was for a long time a Nokia 3310 user) and have some baby to move around in a stroller to decide myself! If you were like me, hesitating, you should not and I will try to explain why and how you can contribute."
  },
  {
    "objectID": "posts/2023-01-06-2-using-go-map-to-contribute-to-osm/index.html#footnotes",
    "href": "posts/2023-01-06-2-using-go-map-to-contribute-to-osm/index.html#footnotes",
    "title": "Using Go Map to contribute to OSM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee this nice blog post on the history of “global street maps”↩︎"
  },
  {
    "objectID": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html",
    "href": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html",
    "title": "Trees in Open Data Databases in France",
    "section": "",
    "text": "I collect tree data sets and here are some links to open data from France. I will update the list when I find more!\nYou can find out more about my work on my github. To extract the trees from the files I needed to do a bit of data wrangling."
  },
  {
    "objectID": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html#guingamp",
    "href": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html#guingamp",
    "title": "Trees in Open Data Databases in France",
    "section": "Guingamp",
    "text": "Guingamp\nTypical lat/long in field.\nguingamp &lt;- sf::st_read(\"sources/Arbres de la Ville de Guingamp.csv\")\nguingamp &lt;- sf::st_as_sf(guingamp, coords = c(\"longitude\", \"latitude\"), crs = 4326)"
  },
  {
    "objectID": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html#ecoline",
    "href": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html#ecoline",
    "title": "Trees in Open Data Databases in France",
    "section": "Ecoline",
    "text": "Ecoline\nIsolated trees are only the ones with “Arbre isolé” in ele_txt field.\niau_ecoline &lt;- sf::st_read(\"sources/elements-fixes-ponctuels-de-la-couche-ecoline-dile-de-france.json\")\niau_ecoline &lt;- iau_ecoline[iau_ecoline$ele_txt == \"Arbre isolé\",]"
  },
  {
    "objectID": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html#nancy-metropolis",
    "href": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html#nancy-metropolis",
    "title": "Trees in Open Data Databases in France",
    "section": "Nancy metropolis",
    "text": "Nancy metropolis\nNancy is a bit more tricky. Each city has a directory where you can found isolated trees (ARBRE_ISOLE.shp). Houdemont, one of the cities, has a file with a different number of columns so we needed to correct this.\nnancy &lt;- sapply(list.files(pattern = \"ARBRE_ISOLE.shp$\", recursive = T), sf::st_read) # read all the file \nhoudemont &lt;- nancy[6] # we get Houdemont's data\nhoudemont$`sources/RESTITUTION_TOPO_3D_SHP/HOUDEMONT/ARBRE_ISOLE.shp`$Z &lt;- NA # add the missing data\nhoudemont$`sources/RESTITUTION_TOPO_3D_SHP/HOUDEMONT/ARBRE_ISOLE.shp` &lt;- \n    houdemont$`sources/RESTITUTION_TOPO_3D_SHP/HOUDEMONT/ARBRE_ISOLE.shp`[    # reorgonize it\n        ,c(\"HAUTEUR\",  \"DIAMETRE\", \"TYPE\", \"CCOCOM\", \"Z\",  \"geometry\")\n    ]\nnancy &lt;- do.call(rbind, nancy[-6]) # one file from every cities except Houdemont \nnancy &lt;- rbind(nancy, houdemont$`sources/RESTITUTION_TOPO_3D_SHP/HOUDEMONT/ARBRE_ISOLE.shp`) # adding Houdemont"
  },
  {
    "objectID": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html#list-of-data-sets",
    "href": "posts/2021-01-10-trees-in-open-data-databases-in-france/index.html#list-of-data-sets",
    "title": "Trees in Open Data Databases in France",
    "section": "List of data sets",
    "text": "List of data sets\n\n\n\nName\nData\n\n\n\n\n“L’arbre à Paris”\nGeoJson\n\n\n“Arbres d’alignement de la Métropole de Lyon”\nshp\n\n\n“Patrimoine arboré de Bordeaux”\nGeoJson\n\n\n“Cartographie des arbres communaux” (Nice)\nGeoJson\n\n\n“Les arbres de Grenoble”\nGeoJson\n\n\n“Arbres d’alignement de Montpellier”\nGeoJson\n\n\n“Arbres” (Grand Paris Seine Ouest)\nGeoJson\n\n\n“Arbres” (Agen)\nGeoJson\n\n\n“ARBRES” (Agglo du Saint-Quentinois)\nGeoJson\n\n\n“Arbres remarquables” (Metz)\nshp\n\n\n“Arbres d’alignement”(Seine-Saint-Denis)\nGeoJson\n\n\n“Arbres d’alignements - Ville de Versailles”\nGeoJson\n\n\n“Arbres dans les parcs de la ville de Versailles”\nGeoJson\n\n\n“Arbres d’alignement” (Nevers)\nshp\n\n\n“Arbres d’ornement” (Nevers)\nshp\n\n\n“Arbres d’alignement - Toulouse”\nGeoJson\n\n\n“Arbres - Ville d’Orléans”\nshp\n\n\n“Les arbres de Saint-Egrève”\nshp\n\n\n“Arbres d’alignement - Bayonne”\nGeoJson\n\n\n“Arbres d’alignement sur la voirie départementale”\nGeoJson\n\n\n“Arbres d’ornement des espaces verts de la Ville de Rennes”\nGeoJson\n\n\n“Arbres d’alignement en accompagnement de voirie sur la ville de Rennes”\nGeoJson\n\n\n“Caractéristiques des arbres d’alignements gérés par la Ville de Mulhouse”\nGeoJson\n\n\n“Arbres inventoriés pour la lutte contre le capricorne asiatique - Foyers de Divonne-les-Bains”\n\n\n\n“Arbres de la Ville de Guingamp”\nGeoJson\n\n\n“Patrimoine arboré ponctuel des voies navigables appartenant à la Région Bretagne”\nGeoJson\n\n\n“Inventaire arboré de Grand Paris Sud”\nGeoJson\n\n\n“Cadastre vert - Les arbres” (Hauts-de-Seine)\nGeoJson\n\n\n“Arbres Alignement sur la CAPP (Pau)”\n\n\n\n“Éléments fixes ponctuels de la couche Ecoline d’Île-de-France”\nGeoJson\n\n\n“Restitution topo 3D” (Nancy)\nshp"
  },
  {
    "objectID": "posts/2023-01-14-4-mapscaping-geospatial-consulting-as-a-business-and-a-career/index.html",
    "href": "posts/2023-01-14-4-mapscaping-geospatial-consulting-as-a-business-and-a-career/index.html",
    "title": "Mapscaping: geospatial consulting – as a business and a career",
    "section": "",
    "text": "The Mapscaping Podcast is a great podcast in the “geospatial” space. It covers a broad diversity of topics and the production value is always top notch.\nI was quite interested in the last episode geospatial consulting – as a business and a career and wanted to share some of the little gems this episode has to offer.\nThe episode was an interview with Todd Slind (VP of Technology) at Locana.co about geospatial consulting. As the title states, the show addresses both the business and career aspect of geoconsulting.\nBefore going into these two aspects, I would like to thank Locana for sponsoring the podcast and contributing to the geospatial community. I am also following Geospatial Connections with another Locana employe and I feel those contributions are great!"
  },
  {
    "objectID": "posts/2023-01-14-4-mapscaping-geospatial-consulting-as-a-business-and-a-career/index.html#geospatial-consulting-as-business",
    "href": "posts/2023-01-14-4-mapscaping-geospatial-consulting-as-a-business-and-a-career/index.html#geospatial-consulting-as-business",
    "title": "Mapscaping: geospatial consulting – as a business and a career",
    "section": "Geospatial Consulting as Business",
    "text": "Geospatial Consulting as Business\n\n“We do not want to be on the bleeding edge […] where we are doing all the bleeding […].” Todd Slind\n\nA lot of the exchange was around the need for a consulting business (and as we will see later how it applies to individuals) to strike a balance between having a strong core or area of expertise while also placing some bets in other sectors.\nThis dichotomy can also be seen in the “mature market” versus “less mature market”. For a consultancy business, it is good to have strong foundations in an established industry. It could be in both markets, but I guess it seems it is easier in a mature market thanks to “off-the-shelf solutions”.\nBoth markets were defined as:\n\nMature Market:\n\nStandardize: allow “off-the-shelf solutions”\n\nPresence of a robust consultancy ecosystem (with competition)\n\nInformed consumers\n\n\n\nLess Mature Market:\n\nProblems are still unclear\n\nNeed for more custom development / more IT\nMore constraints from the environment (ex: low bandwidth)\nNeed to build a common language\n\nThe best way to build a strong core is to practice and build experience."
  },
  {
    "objectID": "posts/2023-01-14-4-mapscaping-geospatial-consulting-as-a-business-and-a-career/index.html#geospatail-consulting-as-a-career",
    "href": "posts/2023-01-14-4-mapscaping-geospatial-consulting-as-a-business-and-a-career/index.html#geospatail-consulting-as-a-career",
    "title": "Mapscaping: geospatial consulting – as a business and a career",
    "section": "Geospatail consulting as a career",
    "text": "Geospatail consulting as a career\nThis leads, at the consultant level, to a trade off between “learning on the job” versus “being in your comfort zone”. You need to maintain your core area of expertise while also being curious about new opportunities.\nAccording to Todd Slind, a great consultant should have:\n- Curiosity/creativity as a self-starter\n- Can “interface” with clients (importance of speaking the same language)\n- Is able to have ownership over their own personal development\nIf a consultant wants to bring new ideas/new technology to improve a process, it is better if they can bring some prototype or find a way to show how it can be “real”. I should print this advice: it is easy to see all the new shiny developments (sedona, arrow) but the better question is “where and how to apply them to solve problems?”\nI also liked the idea for consultants to socialize and create discussion spaces dedicated to estimating the time needed for different assignments.\nStrangely, technology was not brought into the discussion too much (ie: impact of choosing one, “one-way door versus two-way doors”) and I hope this can be a topic for another episode!!!"
  },
  {
    "objectID": "posts/2025-03-12_better-bash-prompt/index.html",
    "href": "posts/2025-03-12_better-bash-prompt/index.html",
    "title": "Improving my default bash prompt",
    "section": "",
    "text": "Even though I am not a system admin, I work frequently in remote or virtual computers other than my desktop, and Ubuntu default terminal requires me to have my username, hostname, and full directory path in my prompt … whiiich tends to be a bit lengthy!\nMost of the time, I use zsh and oh-my-zsh. In 5 minutes, you can set up a nice config and call it a day (and I recommend doing that!).\nFor some obscure reason, I wanted to try doing some linux certifications and since most of them require you to be on bash I wanted to stick with it and have a minimal setup for my daily needs.\nThe prompt was one thing that really bothered me!"
  },
  {
    "objectID": "posts/2025-03-12_better-bash-prompt/index.html#stuff-outside-of-my-needs-but-worth-learning",
    "href": "posts/2025-03-12_better-bash-prompt/index.html#stuff-outside-of-my-needs-but-worth-learning",
    "title": "Improving my default bash prompt",
    "section": "Stuff outside of my needs but worth learning",
    "text": "Stuff outside of my needs but worth learning\nI enjoyed playing with my prompts and testing all the examples that provided! It is totally worth reading and since your use of shell may differ from mine, you will probably pick out other interesting ideas.\nI just want to highlight a few of them here.\n\nA script to test colors on bash!\na very nice way of using local variables in function to make it more readable:\n\n# [...] plenty of code above\nlocal GRAY=\"\\[\\033[1;30m\\]\"\n# [...] then using $GRAY\n\nExamples using PROMPT_COMMAND variables\nUsing functions to switch prompt styles (happy_day_prompt /sad_day_prompt, anyone?)\nDisplaying other system information like number of running jobs, last exit status returned and more!"
  },
  {
    "objectID": "posts/2025-03-12_better-bash-prompt/index.html#stuff-that-i-learned-and-that-i-am-going-to-use",
    "href": "posts/2025-03-12_better-bash-prompt/index.html#stuff-that-i-learned-and-that-i-am-going-to-use",
    "title": "Improving my default bash prompt",
    "section": "Stuff that I learned and that I am going to use",
    "text": "Stuff that I learned and that I am going to use\nIn bash, .bashrc seems to be the place (I am deferring to Giles Orr here) to either source another script with your functions setting your prompt or to directly set your prompt.\nIn Ubuntu it seems my .bashrc already had some lines of codes to handle the prompt (depending if my terminal can handle color or not):\n# color_prompt and debian_chroot are declare a bit above\nif [ \"$color_prompt\" = yes ]; then\n   PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ '\nelse\n   PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ '\nfi\nI think it is easier for a quick implementation to just modify those lines instead of sourcing a specific function.\nYou can also do echo $PS1to check what is yours. If you want to play with it first before updating the file you should follow the same pattern provided by Giles Orr:\nSAVE=$PS1 # \"save\" it\nPS1=\"\\W$ \" # test something\nPS1=$SAVE # back to the original\nIf, like me, you did not grow up with ASCII code and bash Prompt Escape Sequences this is the one you need to understand:\n\n\\033 is an ASCII escape character (it could probably be replaced by \\e in bash)\n01;32m are defining font then color (or background color). I do not know why they should be followed by m. If you do not want to keep the same font/color after your prompt, you need to reverse character to default (\\033[00m).\n\n\n\n\nDaniel Crisman’s script result\n\n\nSee also this wikipedia article for more info: https://en.wikipedia.org/wiki/ANSI_escape_code#Colors\n\n\\[  \\] is the beginning and end sequence to include non-printing characters, a backlash (\\) needs to be used here because [ and ] are parts of reserved characters. Even if we do not see characters defining escape characters and colors they are still taking up space in the prompt (quite a lot!) and so we need a way to indicate that they should not be “displayed”.\n\\u is for the user name (I do not want to keep it)\n\\h for the host name (I also do not want to keep it)\n\\w is for the current working directory and \\W (uppercase) just gives us the basename of the working directory (perfect for me)\n$ needs also to be escaped (\\$), notice also that a white space is included after \\$ ' and should be kept.\n\nAs you see, an easy ways to change my prompt is to remove : the part before it up to the parts that are defined using chroot (on my setup this also could probably be removed, but I may log into places where chroot is specified and I am being restricted!) andchange` for its uppercase version.\nFor the git branch I wrote a quick function:\ngit_branch_show_current () {\ngit branch 2&gt; /dev/null --show-current\nreturn\n}\nIt displays the current branch and sends any error in a dark corner of /dev/null/.\nThen I added some parenthesis to look a bit fancy, changed quotes and changed colors.\ngit_branch_show_current () {\ngit branch 2&gt; /dev/null --show-current\nreturn\n}\n\n\nif [ \"$color_prompt\" = yes ]; then\n   PS1=\"${debian_chroot:+($debian_chroot)}\\[\\033[31;35m\\](\\$(git_branch_show_current))\\033[00m\\]:\\[\\033[01;34m\\]\\W\\[\\033[00m\\]\\$ \"\nelse\n   PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ '\nfi\nI now have:\n\nin purple, surrounded by parenthesis, the git branch branch I am in\n: a separator\nthe basename of the repository (still in blue)\n$"
  },
  {
    "objectID": "posts/2024-09-15_function_factory/index.html",
    "href": "posts/2024-09-15_function_factory/index.html",
    "title": "Function factories to improve Database read and write",
    "section": "",
    "text": "Like a lot of organizations, my team uses databases (DB) to organize and centralize data. To simplify the team’s life and make our code less redundant, we use functions, in an internal R package, that manage connecting to the DB, writing and reading to it (see Emily Riederer’s website for a great post about that idea1).\nI think those functions are great and remove a lot of “boilerplate code” that everyone needs to write. After one year of using them and tinkering with them a bit, I think including them in a function factory could enhance them.\nA function factory is a function that returns a function. My goal is not to present them here but I hope that the use case I want to apply to them to can illustrate their power!\nYou can find out more about them in the excellent Advanced R 2 or for a more targeted use in a RAP 3.\nI will first give you some understanding of our process, then present how function factories may help."
  },
  {
    "objectID": "posts/2024-09-15_function_factory/index.html#our-setup",
    "href": "posts/2024-09-15_function_factory/index.html#our-setup",
    "title": "Function factories to improve Database read and write",
    "section": "Our setup:",
    "text": "Our setup:\nUsually we organize our works in a project and a schema stores all tables related to a project.\nTo be able to access the DB, you need to provide some information (address, port, DB name, credentials, etc ..).\nOur connect DB function is very close to what Emily is advocating\n\nget_database_conn &lt;- function() {\n\nconn &lt;-\n  DBI::dbConnect(\n    drv = odbc::odbc(),\n    driver = \"driver name here\",\n    server = \"server string here\",\n    UID = Sys.getenv(\"DB_USER\"),\n    PWD = Sys.getenv(\"DB_PASS\"),\n    port = \"port number here\"\n  )\n\nreturn(conn)\n}\n\nThe main difference (outside of drivers and slightly different default arguments related to our infrastructure) is that we are using the options argument from dbConnect to specify the search path, specifying that the first schema should be used. (see for PG’s documentation for more here)\nHence our function look like this:\n\nget_db_con &lt;- function(schema) {\n\ncon &lt;-\n  DBI::dbConnect(\n    # [...] similar stuff\n    options = sprintf(\"-c search_path=%s\", paste0(c(schema,\n            \"\\\"$user\\\"\", \"and_more_specific_stuff\")))\n  )\nreturn(con)\n\n}\n\nOnce you have your connection object, you can use it to read a table in memory (usually with a function derived from DBI::dbReadTable), write a table to the DB (DBI::dbWriteTable), get the result of a specific query (DBI::dbGetQuery) or even execute a statement (DBI::dbExecute). DBI4 is truly a work of art!\n\n\n\n\n\n\nTip\n\n\n\nSpecifying Postgresql search_path allow some nice listing with DBI::dbListTables(con)\n\n\nA typical workflow would be like this:\n\ncon &lt;- get_db_con(\"my_project_bill\") # granted the schema exist\n\nsome_data &lt;- DBI::dbReadTable(con, \"a_table\")\nother_data &lt;- DBI::dbReadTable(con, \"other_table\")\n\n# Do some cool stuff with the data\n\nDBI::dbWriteTable(con, \"new_shiny_table\", new_table)\n\nDBI::dbDisconnect(con)"
  },
  {
    "objectID": "posts/2024-09-15_function_factory/index.html#first-step-code",
    "href": "posts/2024-09-15_function_factory/index.html#first-step-code",
    "title": "Function factories to improve Database read and write",
    "section": "First step: code",
    "text": "First step: code\nWith few lines, it is not very problematic but “Do some cool stuff” can be hundreds of lines (or more), they can take more than one session to be made (it could be useful for the data to be cached). This means that code could be functionalized a bit and used in targeted pipeline.\nA first attempt look like this:\n\nget_table &lt;- function(schema, table){\n   con &lt;- get_db_con(schema)\n   on.exit(DBI::dbDisconnect(con), add = TRUE)\n    # add = TRUE is not that needed here\n    # but Hadley Wickham recommend it\n   dat &lt;- DBI::dbReadTable(con, table)\n   return(dat)\n}\n\nwrite_table &lt;- function(schema, name, table, ...) {\n   con &lt;- get_db_con(schema)\n   on.exit(DBI::dbDisconnect(con), add = TRUE)\n   dat &lt;- DBI::dbdbWriteTable(con, name, table, ...)\n   return(dat)\n}\n\n\n\n\n\n\n\nNote\n\n\n\nWe used ... because usually writing to the DB is a bit more complicated and we could use some argument like overwrite for example.\n\n\nThose functions are a good start. Now we have:\n\nsome_data &lt;- get_table(\"my_project_bill\", \"a_table\")\nother_data &lt;- get_table(\"my_project_bill\", \"other_table\")\n\n# Do some cool stuff with the data\n\nwrite_table(\"my_project_bill\", \"new_shiny_table\", new_table)\n\nFrom my experience, those functions will mostly be stored in utility.R files somewhere and “sourced”.\nBy using a function we get some added benefit:\n\non.exit() helps us remember to close the connection\nwe can add message() and use some assertions (stopifnot()) inside those functions to make our work more robust\nslightly less “not very useful” code exposed, remember “Do some cool stuff with the data” is the important part, getting and sending data should be abstracted\nwe are starting to capitalize code"
  },
  {
    "objectID": "posts/2024-09-15_function_factory/index.html#second-try-use-function-factory",
    "href": "posts/2024-09-15_function_factory/index.html#second-try-use-function-factory",
    "title": "Function factories to improve Database read and write",
    "section": "Second try: use function factory",
    "text": "Second try: use function factory\nThere are at least two things that are a bit problematic with our first attempt.\nThe first is that \"my_project_bill\" is repeated a lot. We could be tempted to use some config file, but here I like to follow Miles McBain5’ advice and not do it. The other problem is that we probably have multiple projects that need to write and read tables then we will probably need to write or, worse, copy paste some variation of the same code and copy paste is like a virus (well I guess now we can say “I am copy/pasting code using AI”)\nOur need is to produce code that is generating functions so here it seems function factory would be nice:\n\n# what is a nice naming convention for function factory\ncreate_get_table &lt;- function(schema) {\n    force(schema)\n    function(table_name) {\n        con &lt;- get_db_con(schema)\n        on.exit(DBI::dbDisconnect(con), add = TRUE)\n        DBI::dbReadTable(con, table_name)\n    }\n}\n\ncreate_write_table &lt;- function(schema) {\n    force(schema)\n    function(table_name, dat, ...) {\n        con &lt;- get_db_con(schema)\n        on.exit(DBI::dbDisconnect(con), add = TRUE)\n        DBI::dbWriteTable(con, table_name, dat)\n    }\n}\n\nNow we can start our workflow like this:\n\nget_proj_bill_table &lt;- create_get_table(\"my_project_bill\")\nwrite_proj_bill_table &lt;- create_write_table(\"my_project_bill\")\n\nsome_data &lt;- get_proj_bill_table(\"a_table\")\nother_data &lt;- get_proj_bill_table(\"other_table\")\n\n# Do some cool stuff with the data\n\nwrite_proj_bill_table(\"new_shiny_table\", new_table)\n\nWe have:\n\nremoved the need of a sourced file (avoiding weird debugging),\ndecreased the clutter of using boilerplate code (those functions can be put in the same internal package than our get_db_con)\n\nIn this article, I am only covering “read” and “write” functionality so this can be helpful to colleagues who are more fluent in R or Python than SQL and prefer doing the EDA in those languages (which are way better for generating quick feedback loops and visual data inspection/visualization).\nI am curious how other orgs are implementing internal packages to read and write functions: what are you using in your internal packages to simplify access to databases?"
  },
  {
    "objectID": "posts/2024-09-15_function_factory/index.html#footnotes",
    "href": "posts/2024-09-15_function_factory/index.html#footnotes",
    "title": "Function factories to improve Database read and write",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEmily Riederer. January 21, 2021, https://www.emilyriederer.com/post/team-of-packages/↩︎\nWickham, Hadley. 2015. Advanced R. Boca Raton, FL: CRC Press. http://www.crcnetbase.com/isbn/9781466586970 and https://adv-r.hadley.nz/function-factories.html↩︎\nRodrigues, Bruno. 2023. Building Reproducible Analytical Pipelines with R. https://raps-with-r.dev/.↩︎\nR Special Interest Group on Databases (R-SIG-DB), Wickham H, Müller K (2024). DBI: R Database Interface. R package version 1.2.3, https://github.com/r-dbi/DBI, https://dbi.r-dbi.org.↩︎\nMcBain. 2024, March 11. Before I Sleep: Patterns and anti-patterns of data analysis reuse. Retrieved from https://milesmcbain.com/posts/data-analysis-reuse/↩︎"
  },
  {
    "objectID": "posts/2025-02-06_codeberg_link/index.html",
    "href": "posts/2025-02-06_codeberg_link/index.html",
    "title": "Codeberg links and icons!",
    "section": "",
    "text": "I like GitHub (GH) but it seems like a good idea to learn about what other software forges are out there… !\nYou can find a great post about it here: https://mastodon.social/@gvwilson/113945550159389620\n\n \n\nPost by @gvwilson@mastodon.social\n\n\nView on Mastodon\n\n\n\n\n\nI will add to that:\n\na bit of competition1 is good\ndiscovering other implementations is always worth it.\n\n1 GitLab is another option.While this blog is still on GH I will test Codeberg more! Hence, I wanted to add a link of my, nearly empty, codeberg (adding more nudges for others to try!)\nIn quarto this is quite easy:\n\nDownload the .svg from the codeberg design repo and put it where you are usually add such assets (example: assets/img/)\n\nThe repo is archived but we are complying with the licence and the “Do” section.\n\nUpdate your _quarto.yml with a your take on:\n\n      - text: \"![](/assets/img/codeberg-logo_icon_white.svg)\"\n        href: https://codeberg.org/defuneste\nUsing text was not obvious but this GH discussion (sic) was helpful. Notice that I specified an absolute path. Thanks! @MickaelCanouil@fosstodon.org and @ellakaye@fosstodon.org\nIt should be good to go!\nAnother, easier, option would be to replace text: \"![]your_link\" with ìcon: git."
  }
]